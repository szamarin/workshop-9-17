{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Workloads on SageMaker using Ray\n",
    "\n",
    "In this notebook we will see how we can scale our data processing workloads to multi-node clusters using [Ray](https://www.ray.io/) on Amazon SageMaker. We will use the SageMaker Python SDK to create a training job that uses Ray to distribute the workload across multiple instances. Ray is a distributed computing framework that makes it easy to scale your applications from a single machine to a large cluster.Furthermore, the [modin library](https://github.com/modin-project/modin) provides an pandas API that is scaled using ray. That mean that we can use the familiar pandas API to process large datasets in parallel across multiple instances. The awswrangler library can utilized either pandas or modin as the underlying engine for processing dataframes.\n",
    "\n",
    "<div style=\"border: 1px solid black; padding: 10px; background-color: #ffffcc; color: black;\">\n",
    "<strong>Note:</strong> Make sure to fully run the first notebook to ingest the data into Athena before running this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from sagemaker.pytorch import PyTorch  # PyTorch Estimator for running our training job\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "bucket = sess.default_bucket()  # default bucket name\n",
    "account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load values from the first notebook\n",
    "\n",
    "if not Path(\"lab_values.json\").exists():\n",
    "    raise FileNotFoundError(\"Please run the first notebook first.\")\n",
    "else:\n",
    "    lab_values = json.loads(Path(\"lab_values.json\").read_text())\n",
    "    input_data_location = lab_values[\"s3_csv_folder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = f\"s3://{bucket}/ml_workshop/aggregation-job/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we are not training an ML model, we'll use a training job to leverage the SageMaker infrastructure to run our Ray job. There are several advantages of the training job as compared to a Processing job namely:\n",
    "- Wider range of instance types and sizes\n",
    "- Ability to pass arguments to the script via a python dictionary rather than a list of strings (see the `hyperparameters` argument below)\n",
    "- Ability to keep instances in a waiting state after the job completes for debugging purposes (`keep_alive_period_in_seconds` argument) which allows reusing warm instances for sequential jobs\n",
    "\n",
    "The [compute_aggregations.py](./ray_script/compute_aggregations.py) script will compute simple and more complex aggregations on the data. You can set `USE_RAY` variable to False to see the difference in performance between pandas and modin.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_RAY = True\n",
    "\n",
    "job = PyTorch(\n",
    "    source_dir=\"ray_script\",\n",
    "    entry_point=\"compute_aggregations.py\",\n",
    "    framework_version=\"2.2\",\n",
    "    py_version=\"py310\",\n",
    "    role=role,\n",
    "    environment={\"USE_RAY\": str(USE_RAY)},           # we can pass environment variables to the training job\n",
    "    hyperparameters={                                # hyperparameters are passed as command line arguments to the training script\n",
    "        \"input_data_location\": input_data_location,\n",
    "        \"output_data_location\": output_location,\n",
    "    },\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count = 3 if USE_RAY else 1,            # use 3 instances if Ray is enabled otherwise use 1 instance\n",
    "    max_run=1000,                                    # maximum allowed runtime in seconds\n",
    "    keep_alive_period_in_seconds=300                 # instances will be kept alive for 300 seconds after the job finishes for use in future jobs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker training job is started by calling the fit method\n",
    "job.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $output_location/ --recursive --human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
