{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis, Exploration, and Feature Engineering\n",
    "Now that the data is ingested into Athena we can begin to explore the data and perform some feature engineering. Since the data is provided as a Pandas DataFrame we can use familiar Pandas function and data science libraries to explore and prepare the data.\n",
    "\n",
    "<div style=\"border: 1px solid black; padding: 10px; background-color: #ffffcc; color: black;\">\n",
    "<strong>Note:</strong> Make sure to fully run the first notebook to ingest the data into Athena before running this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqq category-encoders\n",
    "%pip install -Uqq seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "import awswrangler as wr\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wr.engine.set(\"python\")\n",
    "wr.memory_format.set(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load values from the first notebook\n",
    "\n",
    "if not Path(\"lab_values.json\").exists():\n",
    "    raise FileNotFoundError(\"Please run the first notebook first.\")\n",
    "else:\n",
    "    lab_values = json.loads(Path(\"lab_values.json\").read_text())\n",
    "    database_name = lab_values[\"database_name\"]\n",
    "    table_name = lab_values[\"wrangler_parquet_table_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the entire table into a pandas dataframe\n",
    "# you can also pass in the chunksize parameter to read the table in chunks\n",
    "df = wr.athena.read_sql_table(table_name, database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the summary statistics\n",
    "df.describe(percentiles=[0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is provided as a Pandas DataFrame we can use common visualization libraries such as Matplotlib and Seaborn to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(df, column_name):\n",
    "    \"\"\"\n",
    "    Plots the distribution of numeric values in a specified column of a DataFrame.\n",
    "    \"\"\"\n",
    "    sns.displot(df[column_name], kde=True, bins=30)\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df, \"ti_ln_remaining_term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df, \"ti_cu_cust_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(x=\"ti_ln_purpose\", y=\"ti_ln_original_loan_amount\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science libraries such as category_encoders and scikit-learn can be used to perform feature engineering and data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from category_encoders.james_stein import JamesSteinEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "# treat all numeric columns that have more than 50 unique values as continuous\n",
    "numeric_features = df[df.nunique()[df.nunique() > 25].index].select_dtypes(\"number\").columns\n",
    "\n",
    "category_features = [col for col in df.columns if col not in numeric_features and \"date\" not in col]\n",
    "\n",
    "date_features = [col for col in df.columns if \"date\" in col and col not in [\"ti_ln_date_open_year\", \"ti_ln_date_open_month\"]]\n",
    "\n",
    "numeric_preprocessor = ColumnTransformer(transformers = [(\"scaler\", StandardScaler(), numeric_features)], remainder=\"drop\")\n",
    "category_preprocessor = ColumnTransformer(transformers = [(\"encoder\", JamesSteinEncoder(), category_features)], remainder=\"drop\")\n",
    "\n",
    "\n",
    "def extract_date_features(df):\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col + '_year'] = df[col].dt.year\n",
    "            df[col + '_month'] = df[col].dt.month\n",
    "            df[col + '_day'] = df[col].dt.day\n",
    "    return df.drop(columns=[col for col in df.columns if pd.api.types.is_datetime64_any_dtype(df[col])])\n",
    "\n",
    "date_preprocessor = FunctionTransformer(extract_date_features, validate=False)\n",
    "\n",
    "combined_preprocessor = FeatureUnion(transformer_list=[\n",
    "    (\"numeric\", numeric_preprocessor),\n",
    "    (\"categorical\", category_preprocessor),\n",
    "    (\"date\", date_preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_date_features = [f\"{col}_{suffix}\" for col in date_features for suffix in ['year', 'month', 'day']]\n",
    "all_columns = numeric_features.tolist() + category_features + new_date_features\n",
    "transformed_data = combined_preprocessor.fit_transform(df, df[\"ti_ln_balance\"])\n",
    "\n",
    "np.save(\"transformed_data\", transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis with SQL\n",
    "\n",
    "We can also use SQL queries to explore the data. This can be useful for large datasets where it is not feasible to load the entire dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_numeric_sql = f\"\"\"\n",
    "WITH percentiles AS (\n",
    "    SELECT\n",
    "        'ti_ln_remaining_term' AS column_name,\n",
    "        MIN(ti_ln_remaining_term) AS min_value,\n",
    "        approx_percentile(ti_ln_remaining_term, 0.25) AS p25,\n",
    "        approx_percentile(ti_ln_remaining_term, 0.50) AS p50,\n",
    "        approx_percentile(ti_ln_remaining_term, 0.75) AS p75,\n",
    "        MAX(ti_ln_remaining_term) AS max_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_balance' AS column_name,\n",
    "        MIN(ti_ln_balance) AS min_value,\n",
    "        approx_percentile(ti_ln_balance, 0.25) AS p25,\n",
    "        approx_percentile(ti_ln_balance, 0.50) AS p50,\n",
    "        approx_percentile(ti_ln_balance, 0.75) AS p75,\n",
    "        MAX(ti_ln_balance) AS max_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_installment_due' AS column_name,\n",
    "        MIN(ti_ln_installment_due) AS min_value,\n",
    "        approx_percentile(ti_ln_installment_due, 0.25) AS p25,\n",
    "        approx_percentile(ti_ln_installment_due, 0.50) AS p50,\n",
    "        approx_percentile(ti_ln_installment_due, 0.75) AS p75,\n",
    "        MAX(ti_ln_installment_due) AS max_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_val_payments' AS column_name,\n",
    "        MIN(ti_ln_val_payments) AS min_value,\n",
    "        approx_percentile(ti_ln_val_payments, 0.25) AS p25,\n",
    "        approx_percentile(ti_ln_val_payments, 0.50) AS p50,\n",
    "        approx_percentile(ti_ln_val_payments, 0.75) AS p75,\n",
    "        MAX(ti_ln_val_payments) AS max_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_val_interest' AS column_name,\n",
    "        MIN(ti_ln_val_interest) AS min_value,\n",
    "        approx_percentile(ti_ln_val_interest, 0.25) AS p25,\n",
    "        approx_percentile(ti_ln_val_interest, 0.50) AS p50,\n",
    "        approx_percentile(ti_ln_val_interest, 0.75) AS p75,\n",
    "        MAX(ti_ln_val_interest) AS max_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_val_total_fees' AS column_name,\n",
    "        MIN(ti_ln_val_total_fees) AS min_value,\n",
    "        approx_percentile(ti_ln_val_total_fees, 0.25) AS p25,\n",
    "        approx_percentile(ti_ln_val_total_fees, 0.50) AS p50,\n",
    "        approx_percentile(ti_ln_val_total_fees, 0.75) AS p75,\n",
    "        MAX(ti_ln_val_total_fees) AS max_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_final_charge_cycle' AS column_name,\n",
    "        MIN(ti_ln_final_charge_cycle) AS min_value,\n",
    "        approx_percentile(ti_ln_final_charge_cycle, 0.25) AS p25,\n",
    "        approx_percentile(ti_ln_final_charge_cycle, 0.50) AS p50,\n",
    "        approx_percentile(ti_ln_final_charge_cycle, 0.75) AS p75,\n",
    "        MAX(ti_ln_final_charge_cycle) AS max_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    ")\n",
    "SELECT * FROM percentiles;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.athena.read_sql_query(profile_numeric_sql, database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_category_sql = f\"\"\"\n",
    "WITH stats AS (\n",
    "    SELECT\n",
    "        'ti_cu_customer_id' AS column_name,\n",
    "        COUNT(DISTINCT ti_cu_customer_id) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_cu_customer_id) AS missing_values,\n",
    "        CAST((SELECT ti_cu_customer_id FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_cu_customer_id ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_cu_cust_type' AS column_name,\n",
    "        COUNT(DISTINCT ti_cu_cust_type) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_cu_cust_type) AS missing_values,\n",
    "        CAST((SELECT ti_cu_cust_type FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_cu_cust_type ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_cu_num_curr_acct' AS column_name,\n",
    "        COUNT(DISTINCT ti_cu_num_curr_acct) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_cu_num_curr_acct) AS missing_values,\n",
    "        CAST((SELECT ti_cu_num_curr_acct FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_cu_num_curr_acct ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_cu_num_rev_acct' AS column_name,\n",
    "        COUNT(DISTINCT ti_cu_num_rev_acct) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_cu_num_rev_acct) AS missing_values,\n",
    "        CAST((SELECT ti_cu_num_rev_acct FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_cu_num_rev_acct ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_cu_num_mtge_acct' AS column_name,\n",
    "        COUNT(DISTINCT ti_cu_num_mtge_acct) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_cu_num_mtge_acct) AS missing_values,\n",
    "        CAST((SELECT ti_cu_num_mtge_acct FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_cu_num_mtge_acct ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_cu_num_loan_acct' AS column_name,\n",
    "        COUNT(DISTINCT ti_cu_num_loan_acct) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_cu_num_loan_acct) AS missing_values,\n",
    "        CAST((SELECT ti_cu_num_loan_acct FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_cu_num_loan_acct ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_cu_num_dep_acct' AS column_name,\n",
    "        COUNT(DISTINCT ti_cu_num_dep_acct) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_cu_num_dep_acct) AS missing_values,\n",
    "        CAST((SELECT ti_cu_num_dep_acct FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_cu_num_dep_acct ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_account_id' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_account_id) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_account_id) AS missing_values,\n",
    "        CAST((SELECT ti_ln_account_id FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_account_id ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_reason_closed' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_reason_closed) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_reason_closed) AS missing_values,\n",
    "        CAST((SELECT ti_ln_reason_closed FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_reason_closed ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_num_parties' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_num_parties) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_num_parties) AS missing_values,\n",
    "        CAST((SELECT ti_ln_num_parties FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_num_parties ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_account_type' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_account_type) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_account_type) AS missing_values,\n",
    "        CAST((SELECT ti_ln_account_type FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_account_type ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_purpose' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_purpose) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_purpose) AS missing_values,\n",
    "        CAST((SELECT ti_ln_purpose FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_purpose ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_original_term' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_original_term) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_original_term) AS missing_values,\n",
    "        CAST((SELECT ti_ln_original_term FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_original_term ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_original_loan_amount' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_original_loan_amount) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_original_loan_amount) AS missing_values,\n",
    "        CAST((SELECT ti_ln_original_loan_amount FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_original_loan_amount ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_payment_frequency' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_payment_frequency) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_payment_frequency) AS missing_values,\n",
    "        CAST((SELECT ti_ln_payment_frequency FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_payment_frequency ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_payment_method' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_payment_method) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_payment_method) AS missing_values,\n",
    "        CAST((SELECT ti_ln_payment_method FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_payment_method ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_val_fees' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_val_fees) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_val_fees) AS missing_values,\n",
    "        CAST((SELECT ti_ln_val_fees FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_val_fees ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_block_code' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_block_code) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_block_code) AS missing_values,\n",
    "        CAST((SELECT ti_ln_block_code FROM \"{database_name}\".\"{table_name}\"GROUP BY ti_ln_block_code ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_val_arrears' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_val_arrears) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_val_arrears) AS missing_values,\n",
    "        CAST((SELECT ti_ln_val_arrears FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_val_arrears ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_num_mths_in_arrears' AS column_name,\n",
    "        COUNT(DISTINCT ti_ln_num_mths_in_arrears) AS unique_values,\n",
    "        COUNT(*) - COUNT(ti_ln_num_mths_in_arrears) AS missing_values,\n",
    "        CAST((SELECT ti_ln_num_mths_in_arrears FROM \"{database_name}\".\"{table_name}\" GROUP BY ti_ln_num_mths_in_arrears ORDER BY COUNT(*) DESC LIMIT 1) AS VARCHAR) AS most_common_value\n",
    "    FROM \"{database_name}\".\"{table_name}\"\n",
    ")\n",
    "SELECT * FROM stats;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.athena.read_sql_query(profile_category_sql, database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close kernel to release memory\n",
    "# disregard the error message\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
