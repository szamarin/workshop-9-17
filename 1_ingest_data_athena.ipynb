{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion and Preparation\n",
    "In this notebook we will cover a number of patterns for ingesting data into native AWS services and make them accessible via SQL queries and pandas dataframes. We will cover the following tools and services:\n",
    "- [AWS S3:]() A scalable object storage service that can be used to store and retrieve data.\n",
    "- [AWS Glue:]() Provides a data catalog that can be used to discover and search available data sets.\n",
    "- [AWS Athena:]() An interactive query service that can be used to query data stored in S3 using SQL.\n",
    "- [AWS SDK for Pandas (awswrangler):](https://aws-sdk-pandas.readthedocs.io/en/stable/) A library that can be used to query data stored in various AWS data sources and return the results as pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version of awswrangler\n",
    "%pip install -Uqq awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the Data\n",
    "We will use a sample synthetic dataset. First, we will download the data and store it in our notebook instance. We will then upload the data to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sagemaker  # AWS SageMaker Python SDK makes it easier to work with various SageMaker APIs\n",
    "import awswrangler as wr\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "wr.engine.set(\"python\")\n",
    "wr.memory_format.set(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"fico_ml_workshop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 rm --recursive s3://{bucket}/{prefix}/ > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload local csv file to S3\n",
    "s3_csv_data = sagemaker_session.upload_data(\"data/ln_large.csv\", bucket, prefix + \"/data/csv\")\n",
    "print(\"Data uploaded to \" + s3_csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "In this section we will explore 2 approaches for ingesting data into Amazon Athena:\n",
    "1. **Using the awswrangler library:** This is a simpler approach where we will simply read the raw csv data from S3, clean it up a bit using pandas, and then convert it into a parquet format and make it accessible via Athena.\n",
    "2. **Using Athena DDL:** This is a more complex approach where we will create a table in Athena using DDL statements and then query the data. The advantage of this approach is that all of the compute is done on the Athena side and we can query the data without having to download it to our notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest data into Athena using awswrangler\n",
    "\n",
    "With this approach, we'll use the awswrangler library to create a new database. We'll then read the raw csv data from S3, clean it up a bit using pandas, and then convert it into a parquet format.\n",
    "\n",
    "**Advantages**\n",
    "- Simple to use\n",
    "- No need to write DDL statements\n",
    "- Uses pandas for data manipulation\n",
    "\n",
    "**Disadvantages**\n",
    "- Data is downloaded to the notebook instance\n",
    "- Larger datasets may not fit in memory and would require larger instances or running on a cluster\n",
    "- Could be slower for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"workshop\"\n",
    "wrangler_parquet_table_name = \"loan_data_parquet_wrangler\"\n",
    "\n",
    "# create a new database if it doesn't exist\n",
    "if database_name not in wr.catalog.databases().values:\n",
    "    wr.catalog.create_database(name=database_name)\n",
    "\n",
    "# read the CSV file from S3\n",
    "data = wr.s3.read_csv(path=[s3_csv_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For best performance, it is recommended to partition the data on columns that are frequently used in queries. This will allow Athena to skip reading unnecessary data when executing queries. We'll partition by the year and month in which the account was opened. We can partition on a more granular column such as the date the account was opened, however this has the risk of creating too many partitions with very small files which would hurt performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the columns containing the word \"DATE\" to datetime\n",
    "date_cols = [col for col in data.columns if \"DATE\" in col]\n",
    "print(f\"Converting {date_cols} to datetime\")\n",
    "for col in date_cols:\n",
    "    data[col] = pd.to_datetime(data[col], errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "\n",
    "# create additional columns for year and month the account opened \n",
    "# We will use these columns to partition the data in the next step\n",
    "data[\"TI_LN_DATE_OPEN_YEAR\"] = data[\"TI_LN_DATE_OPEN\"].dt.year\n",
    "data[\"TI_LN_DATE_OPEN_MONTH\"] = data[\"TI_LN_DATE_OPEN\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data to S3 in Parquet format and create a table in the Glue Data Catalog\n",
    "s3_output_path = f\"s3://{bucket}/{prefix}/data/wrangler/parquet/\"\n",
    "wr.s3.to_parquet(\n",
    "    df=data,\n",
    "    path=s3_output_path,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\",\n",
    "    database=database_name,\n",
    "    table=wrangler_parquet_table_name,\n",
    "    partition_cols=[\"TI_LN_DATE_OPEN_YEAR\", \"TI_LN_DATE_OPEN_MONTH\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now validate that the table was created\n",
    "tables = [tbl[\"Name\"] for tbl in  wr.catalog.get_tables(database=database_name)]\n",
    "print(f\"Tables in database {database_name}: {tables}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the data\n",
    "wr.athena.read_sql_query(f\"SELECT * FROM {wrangler_parquet_table_name} LIMIT 5\", database=database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest using Athena API and SQL\n",
    "In this approach we will create a table on top of the existing CSV file in S3 using Athena DDL statements. We will then use Athena to convert the data into a parquet format, and query the data.\n",
    "\n",
    "**Advantages**\n",
    "- No need to download the data to the notebook instance\n",
    "- No need to load the data into memory\n",
    "- We can use tiny instances even for large datasets as all work is done on the Athena side\n",
    "- Use SQL to wrangle and query the data\n",
    "\n",
    "**Disadvantages**\n",
    "- Requires knowledge of Athena SQL \n",
    "- May fail if the raw csv has data quality issues\n",
    "- Wrangling with SQL can be more complex and less flexible than using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automatically construct the SQL DDL statement, we will read a small sample of the CSV using pandas and infer the schema. We will then use this schema to create the table in Athena. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(s3_csv_data, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below we wil use a few functions from awswrangler to generate a schema for our dataset and create a table directly over the CSV file in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awswrangler provides a utility function to convert pandas data types to Athena data types\n",
    "schema = wr.catalog.extract_athena_types(sample_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_table_name = \"loan_data_csv\"\n",
    "\n",
    "# we have to pass in the folder containing the CSV files rather than the file itself\n",
    "s3_csv_folder = os.path.dirname(s3_csv_data)\n",
    "\n",
    "wr.catalog.create_csv_table(\n",
    "    database=database_name,\n",
    "    table=csv_table_name,\n",
    "    path=s3_csv_folder,\n",
    "    columns_types=schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate that the table was created\n",
    "tables = [tbl[\"Name\"] for tbl in  wr.catalog.get_tables(database=database_name)]\n",
    "print(f\"Tables in database {database_name}: {tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the csv table\n",
    "csv_table_sample = wr.athena.read_sql_query(f\"SELECT * FROM {csv_table_name} LIMIT 5\", database=database_name)\n",
    "csv_table_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV table has an issue. All of the date columns are stored as string as seen below. Let's fix this by writing a SQL query to convert the date columns to date format. We will then materialize the output of the query into a new optimized parquet based table in Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types of the date columns\n",
    "# We will convert the string to dates but ignore the Int64 ti_ln_write_off_date column for now\n",
    "csv_table_sample.dtypes.filter(like=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cleanup_sql_query(table_name, database_name, columns, partition_col):\n",
    "    \n",
    "    \"helper function to try cast the string date columns to date\"\n",
    "    \n",
    "    query = \"select\\n\"\n",
    "    for col in columns:\n",
    "        if \"date\" in col and col != \"ti_ln_write_off_date\":\n",
    "            query += f\"     TRY_CAST({col} AS DATE) AS {col},\\n\"\n",
    "        else:\n",
    "            query += f\"     {col},\\n\"\n",
    "    \n",
    "    # add partition columns\n",
    "    query += f\"     YEAR(TRY_CAST({partition_col} AS DATE)) AS {partition_col}_year,\\n\"\n",
    "    query += f\"     MONTH(TRY_CAST({partition_col} AS DATE)) AS {partition_col}_month\\n\"\n",
    "    \n",
    "    query += f\"from {table_name}\\n\"\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_sql_query = generate_cleanup_sql_query(\n",
    "    csv_table_name, database_name, csv_table_sample.columns, \"ti_ln_date_open\"\n",
    ")\n",
    "\n",
    "print(cleanup_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's validate that the query works\n",
    "wr.athena.read_sql_query(cleanup_sql_query + \"\\n limit 10\", database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `create_ctas_table` function from awswrangler to create a new table in Athena using a SQL query. **CTAS** stands for Create Table As Select. This function will execute the query and store the results in a new table in Athena.\n",
    "The newly added `ti_ln_date_open_year`, `ti_ln_date_open_month` will be used to partition the data by the year and month the account was opened. This will allow Athena to skip reading unnecessary data when executing queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_parquet_table_name = \"loan_data_parquet_sql\"\n",
    "sql_parquet_output_path = f\"s3://{bucket}/{prefix}/data/sql/parquet/\"\n",
    "\n",
    "!aws s3 rm --recursive {sql_parquet_output_path}\n",
    "wr.catalog.delete_table_if_exists(database=database_name, table=sql_parquet_table_name)\n",
    "\n",
    "ctas_query= wr.athena.create_ctas_table(\n",
    "    sql=cleanup_sql_query,\n",
    "    database=database_name,\n",
    "    ctas_table=sql_parquet_table_name,\n",
    "    s3_output=sql_parquet_output_path,\n",
    "    storage_format=\"PARQUET\",\n",
    "    partitioning_info=[\"ti_ln_date_open_year\", \"ti_ln_date_open_month\"],\n",
    ")\n",
    "\n",
    "wr.athena.wait_query(query_execution_id=ctas_query[\"ctas_query_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [tbl[\"Name\"] for tbl in  wr.catalog.get_tables(database=database_name)]\n",
    "print(f\"Tables in database {database_name}: {tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the data\n",
    "wr.athena.read_sql_query(f\"SELECT * FROM {sql_parquet_table_name} LIMIT 5\", database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a quick benchmark to compare the performance of a query on the raw CSV table and the two parquet tables. You likely won't see a significant difference given the small amount of data and the large amount of time it takes to send the query results back to the notebook instance. However, for larger datasets, the difference in performance can be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 3\n",
    "wr.athena.read_sql_query(f\"\"\"SELECT ti_cu_customer_id, \n",
    "                         ti_ln_account_id, \n",
    "                         count(*) as count \n",
    "                         FROM {csv_table_name} \n",
    "                         where ti_ln_original_term = ti_ln_remaining_term \n",
    "                         GROUP BY 1,2\"\"\", \n",
    "                         database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 3\n",
    "wr.athena.read_sql_query(f\"\"\"SELECT ti_cu_customer_id, \n",
    "                         ti_ln_account_id, \n",
    "                         count(*) as count \n",
    "                         FROM {wrangler_parquet_table_name} \n",
    "                         where ti_ln_original_term = ti_ln_remaining_term \n",
    "                         GROUP BY 1,2\"\"\", \n",
    "                         database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 3\n",
    "wr.athena.read_sql_query(f\"\"\"SELECT ti_cu_customer_id, \n",
    "                         ti_ln_account_id, \n",
    "                         count(*) as count \n",
    "                         FROM {sql_parquet_table_name} \n",
    "                         where ti_ln_original_term = ti_ln_remaining_term \n",
    "                         GROUP BY 1,2\"\"\", \n",
    "                         database=database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the file sizes of the raw CSV table and the two parquet tables. You should see a significant reduction in file size for the parquet tables. Since Athena charges based on the amount of data scanned, this can result in significant cost savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get size of csv data in MB\n",
    "!aws s3api list-objects-v2 --bucket $bucket --prefix $prefix/data/csv --query \"Contents[].Size\" --output json | jq '. | add / (1024 * 1024)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get size of parquet data created with pandas in MB\n",
    "!aws s3api list-objects-v2 --bucket $bucket --prefix $prefix/data/wrangler --query \"Contents[].Size\" --output json | jq '. | add / (1024 * 1024)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get size of parquet data created with sql in MB\n",
    "!aws s3api list-objects-v2 --bucket $bucket --prefix $prefix/data/sql --query \"Contents[].Size\" --output json | jq '. | add / (1024 * 1024)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lab values to a json file for use in the next notebook\n",
    "with open(\"lab_values.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"database_name\": database_name,\n",
    "        \"s3_csv_folder\": s3_csv_folder,\n",
    "        \"csv_table_name\": csv_table_name,\n",
    "        \"wrangler_parquet_table_name\": wrangler_parquet_table_name,\n",
    "        \"sql_parquet_table_name\": sql_parquet_table_name,\n",
    "        \"parquet_output_path\": s3_output_path,\n",
    "    },f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook we covered two approaches for ingesting data into Athena. The first approach used the awswrangler library to read the raw CSV data from S3, clean it up using pandas, and then convert it into a parquet format. The second approach used Athena DDL statements to create a table on top of the existing CSV file in S3 and then convert the data into a parquet format. We then compared the performance and file sizes of the raw CSV table and the two parquet tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
