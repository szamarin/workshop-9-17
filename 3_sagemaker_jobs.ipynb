{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SageMaker Jobs\n",
    "In this notebook, we will introduce you to the concept of SageMaker Jobs. Jobs allow us to scale and productionalize our ML and data processing workflows. We will cover the following topics in this notebook:\n",
    "1. Going from a notebook to a job\n",
    "2. Bringing your own libraries and code\n",
    "3. Using the SageMaker SDK to create jobs\n",
    "4. Using `@remote` decorators to convert functions to jobs\n",
    "\n",
    "<div style=\"border: 1px solid black; padding: 10px; background-color: #ffffcc; color: black;\">\n",
    "<strong>Note:</strong> Make sure to fully run the first notebook to ingest the data into Athena before running this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the exercise here, we'll assume that you want to process data using SQL but not necessarily ingest into Athena as we did in the previous notebook. We'll use [DuckDB](https://duckdb.org/) to process the data, which you can think of as a local SQL engine that can be used to analyze and wrangle large amounts of data.\n",
    "\n",
    "After experimenting with DuckDB in the notebook, we'll convert the code to a job and run it on SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by installing duckdb\n",
    "%pip install -Uqq duckdb\n",
    "%pip install -Uqq duckdb-engine\n",
    "%pip install -Uqq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DuckDB uses uses files to store data, so we create a new database by creating a `.duckdb` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# connect to an existing database, or create one if it doesn't exist\n",
    "conn = duckdb.connect(\"loan_data.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can query data directly from a csv file without loading it into a database\n",
    "sample_df = conn.execute(\"SELECT * FROM 'data/ln_large.csv' LIMIT 5\").df()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also validate how well DuckDB inferred the  data types from the CSV file\n",
    "sample_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for better performance, we can ingest the CSV file into a table within the database\n",
    "conn.execute(\"create table if not exists loan_data as select * from 'data/ln_large.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate that the table was created\n",
    "# .df() returns a pandas DataFrame\n",
    "conn.execute(\"show tables\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now query the data from the table\n",
    "conn.execute(\"select count(*) from loan_data\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a more complex query to profile the numeric columns\n",
    "profile_numeric_sql = \"\"\"\n",
    "WITH percentiles AS (\n",
    "    SELECT\n",
    "        'ti_ln_remaining_term' AS column_name,\n",
    "        MIN(ti_ln_remaining_term) AS min_value,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY ti_ln_remaining_term) AS p25,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ti_ln_remaining_term) AS p50,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ti_ln_remaining_term) AS p75,\n",
    "        MAX(ti_ln_remaining_term) AS max_value\n",
    "    FROM loan_data\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_balance' AS column_name,\n",
    "        MIN(ti_ln_balance) AS min_value,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY ti_ln_balance) AS p25,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ti_ln_balance) AS p50,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ti_ln_balance) AS p75,\n",
    "        MAX(ti_ln_balance) AS max_value\n",
    "    FROM loan_data\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_installment_due' AS column_name,\n",
    "        MIN(ti_ln_installment_due) AS min_value,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY ti_ln_installment_due) AS p25,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ti_ln_installment_due) AS p50,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ti_ln_installment_due) AS p75,\n",
    "        MAX(ti_ln_installment_due) AS max_value\n",
    "    FROM loan_data\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_val_payments' AS column_name,\n",
    "        MIN(ti_ln_val_payments) AS min_value,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY ti_ln_val_payments) AS p25,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ti_ln_val_payments) AS p50,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ti_ln_val_payments) AS p75,\n",
    "        MAX(ti_ln_val_payments) AS max_value\n",
    "    FROM loan_data\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_val_interest' AS column_name,\n",
    "        MIN(ti_ln_val_interest) AS min_value,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY ti_ln_val_interest) AS p25,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ti_ln_val_interest) AS p50,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ti_ln_val_interest) AS p75,\n",
    "        MAX(ti_ln_val_interest) AS max_value\n",
    "    FROM loan_data\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_val_total_fees' AS column_name,\n",
    "        MIN(ti_ln_val_total_fees) AS min_value,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY ti_ln_val_total_fees) AS p25,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ti_ln_val_total_fees) AS p50,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ti_ln_val_total_fees) AS p75,\n",
    "        MAX(ti_ln_val_total_fees) AS max_value\n",
    "    FROM loan_data\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        'ti_ln_final_charge_cycle' AS column_name,\n",
    "        MIN(ti_ln_final_charge_cycle) AS min_value,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY ti_ln_final_charge_cycle) AS p25,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ti_ln_final_charge_cycle) AS p50,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY ti_ln_final_charge_cycle) AS p75,\n",
    "        MAX(ti_ln_final_charge_cycle) AS max_value\n",
    "    FROM loan_data\n",
    ")\n",
    "SELECT * FROM percentiles;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(profile_numeric_sql).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use duckdb to convert the data to parquet format for better performance and interoperability\n",
    "conn.execute(\n",
    "    \"\"\"copy (select *, \n",
    "    year(TI_LN_DATE_OPEN) as TI_LN_DATE_OPEN_YEAR, \n",
    "    month(ti_ln_date_open) as TI_LN_DATE_OPEN_MONTH \n",
    "    from loan_data) \n",
    "    to 'parquet_output' \n",
    "    (FORMAT PARQUET, PARTITION_BY (TI_LN_DATE_OPEN_YEAR, TI_LN_DATE_OPEN_MONTH), OVERWRITE_OR_IGNORE true)\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a SageMaker Processing Job\n",
    "\n",
    "Now let's convert the code to a SageMaker Processing Job. We'll use the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) to create a processing job. The SDK provides a high-level interface for SageMaker Processing Jobs, which allows you to easily create, configure, and run processing jobs.\n",
    "\n",
    "SageMaker includes 3 types of jobs:\n",
    "- [Training Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html): Get's training data from S3, trains a model, and saves the model back to S3.\n",
    "- [Processing Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html): Runs a processing script on input data from S3 and saves the output to S3.\n",
    "- [Batch Transform Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html): Runs a model on input data from S3 and saves the predictions to S3.\n",
    "\n",
    "We will work with processing jobs and training jobs in this notebook.\n",
    "\n",
    "SagMaker Jobs are built around containers and scripts. Users can bring their own containers or leverage the SageMaker provided containers. The SageMaker Python SDK provides a high-level interface for SageMaker Jobs, which allows you to easily create, configure, and run jobs.\n",
    "\n",
    "We will use a `PyTorch` container for this example. Even though we are not using PyTorch, the container behind it is frequently updated and maintained by AWS given the popularity of the framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3                                                            # AWS SDK for Python                                                \n",
    "import json\n",
    "import sagemaker                                                        # SageMaker Python SDK                    \n",
    "from pathlib import Path\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor               # Processor for processing data using the PyTorch farmework container\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput      # ProcessingInput and ProcessingOutput objects for specifying location of input and output data\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "bucket = sess.default_bucket()  # default bucket name\n",
    "account_id = sess.account_id() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load values from the first notebook\n",
    "\n",
    "if not Path(\"lab_values.json\").exists():\n",
    "    raise FileNotFoundError(\"Please run the first notebook first.\")\n",
    "else:\n",
    "    lab_values = json.loads(Path(\"lab_values.json\").read_text())\n",
    "    s3_csv_data = lab_values[\"s3_csv_folder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the PyTorch processor\n",
    "processor = PyTorchProcessor(\n",
    "    framework_version='2.2',          # PyTorch version\n",
    "    py_version='py310',               # Python version\n",
    "    role=role,                        # permissions the processing job will assume\n",
    "    instance_type='ml.m5.xlarge',     # instance type for the processing job (see here for available instances https://aws.amazon.com/sagemaker/pricing/)\n",
    "    instance_count=1,                 # number of instances for the processing job\n",
    "    base_job_name='processing-job'    # name of the processing job\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure the processing inputs and outputs. The `ProcessingInput` and `ProcessingOutput` provide the source and destination of the input and output datasets. The data will be copied into the instance folder or S3 location specified in the `ProcessingInput` and `ProcessingOutput` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_output_location = f\"s3://{bucket}/ml_workshop/data/processing_output\"\n",
    "\n",
    "job_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name=\"data\",\n",
    "        source=s3_csv_data,                     # the S3 location from where the data will be read and copied to the processing instance\n",
    "        destination=\"/opt/ml/processing/input\", # the folder inside the processing instance where the data will be copied to\n",
    "    )\n",
    "]\n",
    "\n",
    "job_outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name=\"data_structured\",\n",
    "        source=\"/opt/ml/processing/output\",   # the folder inside the processing instance where script the output will be written to\n",
    "        destination=s3_output_location,       # the S3 location where the output will be stored\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it's time to run the job. We provide a custom script [convert_to_parquet.py](./processing_script/convert_to_parquet.py). Click the link and take a look at the script. It takes command line arguments for the input and output directories so it knows where to read the data from and where to write the output. Additionally, `processing_script` source directory contains the [requirements.txt](./processing_script/requirements.txt) file which specifies the dependencies for the script, in this case duckdb. If a requirements file is provided in the source directory, SageMaker will install the dependencies before running the script which makes it really easy to bring your own code and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = processor.run(\n",
    "    code=\"convert_to_parquet.py\",          # the script to be run\n",
    "    source_dir=\"processing_script\",        # the folder containing the script\n",
    "    inputs=job_inputs,\n",
    "    outputs=job_outputs,\n",
    "    arguments=[                            # arguments to be passed to the script\n",
    "        \"--input_dir\",\n",
    "        \"/opt/ml/processing/input\",\n",
    "        \"--output_dir\",\n",
    "        \"/opt/ml/processing/output\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the output was written to the specified S3 location\n",
    "!aws s3 ls $s3_output_location/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating jobs using a @remote decorator\n",
    "An alternative and somewhat simpler approach to creating a job is using the [@remote decorator](https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). The `@remote` decorator allows you to convert a function to a job. The decorator takes care of packaging the function and dependencies, uploading the code to S3, and running the job. This is a great way to quickly convert a function to a job without having to write a separate script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.remote_function import remote\n",
    "from sagemaker import image_uris\n",
    "\n",
    "\n",
    "# we will use the PyTorch framework container for the processing job\n",
    "# The remote decorator will actually try to reproduce the environment in which the function was defined so providing the image_uri and dependencies is optional\n",
    "image_uri = image_uri = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    image_scope=\"training\",\n",
    "    region=region,\n",
    "    version=\"2.2\",\n",
    "    py_version=\"py310\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "\n",
    "\n",
    "@remote(\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    dependencies=\"processing_script/requirements.txt\",      # try removing the image uri and dependencies to see if the function still works!\n",
    "    image_uri=image_uri,\n",
    ")\n",
    "def convert_to_parquet(input_s3_path: str, output_s3_path: str):\n",
    "\n",
    "    \"\"\"Takes in s3 path to a CSV file and converts it to parquet format and outputs it to another S3 location\"\"\"\n",
    "    \n",
    "    conn = duckdb.connect(\"temp_data.duckdb\")\n",
    "\n",
    "    # configure S3 access\n",
    "    conn.execute(\n",
    "        \"\"\"CREATE SECRET s3_access (\n",
    "           TYPE S3,\n",
    "           PROVIDER CREDENTIAL_CHAIN\n",
    "        );\"\"\"\n",
    "    )\n",
    "\n",
    "    # create a temporary table from data in S3\n",
    "    conn.execute(f\"CREATE TABLE temp_table AS SELECT * FROM '{input_s3_path}/*.csv'\")\n",
    "\n",
    "    # convert the data to parquet format\n",
    "    conn.execute(\n",
    "        f\"\"\"copy (select *, \n",
    "    year(TI_LN_DATE_OPEN) as TI_LN_DATE_OPEN_YEAR, \n",
    "    month(ti_ln_date_open) as TI_LN_DATE_OPEN_MONTH \n",
    "    from temp_table) \n",
    "    to '{output_s3_path}' \n",
    "    (FORMAT PARQUET, PARTITION_BY (TI_LN_DATE_OPEN_YEAR, TI_LN_DATE_OPEN_MONTH), OVERWRITE_OR_IGNORE true)\"\"\"\n",
    "    )\n",
    "\n",
    "    return output_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_s3_output = f\"s3://{bucket}/ml_workshop/data/processing_output_func\"\n",
    "convert_to_parquet(s3_csv_data, func_s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the output was written to the specified S3 location\n",
    "!aws s3 ls $func_s3_output/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-paced exercises\n",
    "\n",
    "Using any of the approaches above, create a SageMaker job that does the following:\n",
    "   - a. Check that the loan open date (TI_LN_DATE_OPEN) is before the first installment date (TI_LN_DATE_FIRST_INSTALLMENT) and before the closing date (TI_LN_DATE_CLOSED)\n",
    "   - b. Check if customers have as many accounts as the field  TI_CU_NUM_LOAN_ACCT states\n",
    "   - c. Check that all accounts with a close reason (TI_LN_REASON_CLOSED) have a valid close date (TI_LN_DATE_CLOSED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook, we learned how to convert a notebook to a job and run it on SageMaker. We also learned how to use the `@remote` decorator to convert a function to a job.\n",
    "\n",
    "**There's more**\n",
    "\n",
    "SageMaker also offers a [@step](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-step-decorator.html) decorator that allows you to combine multiple functions into a pipeline. This is a great way to create complex workflows that involve multiple steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
